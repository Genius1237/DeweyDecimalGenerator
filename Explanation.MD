# Aim of the Project
A unique code is assigned to every book in the library based on existing schemes. The code given to books is such that similar books have similar codes. Thus similar books can be grouped together and placed in the library. The process of assigning these codes is done manually by the librarian. The librarian goes through and gets the answer to a list of questions about the book. Based on the answers to these questions, the librarian assigns the code to the book. DDC is one such scheme used for assigning codes for the books. This project aims to create a system that will take inputs which are answers to these questions and automatically generate the DDC code for the book.

# Introduction to the DDC
The Dewey Decimal Classification(DDC)[1] is one of the oldest library classification systems that exist, first published in 1876 by Melvil Dewey in the United States. Since it's creation, it has been heavily copyrighted, with the rights held by the Online Computer Library Centre(OCLC), and getting a hardcopy is difficult, as well as expensive. No digital copy exists. Owing to these factors, there exists no computer system that can automatically generate the DDC code given a book. Alternatives to DDC are the UDC(Universal Library Classification), Library of Congress Clssification and Colon Classification.

The DDC schedules describe how a code is given to a book. Each code has 2 parts. The first part is a 3 digit number plus a fractional part. The second part is a continuation of the fractional part. The whole number part is of a fixed length of 3, and the fractional part is of a varying length. The first part is describes the main topic of the book and is derived from the main schedules. This part is built up in an incremental manner, with the left most digit(hundreds) specifying a general category, and as one moves to the right, the category denoted by the digits cumulatively, becomes more and more specific. The second part is derived from auxiliary tables and denotes other details about the book, details which are not central to the theme of the book, but still give some details about the book.

# Literature Review
The DDC system is a proprietary system, and the only existing publication of it is a book, of which a proper digital edition is also not here. This is probably why this kind of a classification task has not been attempted before. Very little literature is also available on this DDC scheme, most of them just describing how to read the tables and generate the code for these books.

# Tools Used
All the code for this was written in python 3, specifically python 3.4. Other than python's inbuilt packages, the following were used: tesserocr, pillow and nltk. NLTK[2] stands for Natural Language Toolkit. NLTK contains many tools used in natural language processing.  It has many NLP algorithms already implemented. NLTK's corpora and training data was also used. Pillow is a fork of PIL, python imaging library, and this was used to store images. Tesserocr was used for text recognition. Tesseract[3] and it's english language training data was installed via the package manager as well. The system used was running CentOS 7 x86_64. It's specifications were Core i5-4570, Quad Core without Hyperthreading, 3.2GHz, 4GB DDR3 RAM, 500GB 7200rpm HDD.

# Methodology
## 1.  Extracting the training data - 
The required training data was present in the 4th volume of the 20th edition of the DDC book. Volume 4 is called the relative index, and it contains a list of classes in alphabetic order along with each class's code. With difficulty, this book was scanned using a high resolution scanner, all 976 pages of it. The resulting PDF was then converted to images, one PNG file for each page. Removing the unnecessary pages resulted in 726 PNG files remaining. The quality of the PNG export was chosen such that it was neither too large, which would make the processing take time, nor too small, which would result in the text present in it being unreadable. It was observed that each page has 2 columns of text. Pillow, a fork of the PIL(Python Imaging Library) was used to process the images as mentioned below. Each image was then cropped so as to remove the header and footer, and then the image was split into 2 halves, right and left. Due to the way the page was positioned while being scanned, the limits for the splitting and crop had to be changed for odd and even pages. Even after specifying these limits, some of the pages were not cropper properly due to the random nature of positioning pages while scanning.
These scanned images were then fed to an open source optical character recognition tool called Tesseract OCR. This is distributed under the Apache license v2.0, making it entirely free to use without providing the code for where it was used. Also distributed is English language training data for tesseract, and this was also used. Tesseract was given each cropped half of a page, and instructed to process the page in the PageSegmentMode of PSM.SINGLE_BLOCK, which treats the entire page as a single block of text and proceeds. This gives the text in each image, albeit with a lot of whitespace in between. Also, in the images, some of the data is ordered hierarchically, using indentation. This hierarchy needed to be captured. Hence an algorithm was written that would remove the whitespace, junk characters, process the hierarchy and get a mapping of class name with DDC code. This would give us a python dictionary with such a mapping. Although the algorithm worked very well for many pages, some miscellaneous characters being detected by the character recognition tool. This algorithm keeps track of the first letter in each page, so as to know which letter’s pages are being processed. Due to this, the description of many classes would get added up to other classes. This would affect the overall performance of the classifier, as will be mentioned later. This entire process takes around 5 seconds per page.

## 2. Training the classifiers - 
From the previous step, a list of classes along with their code was obtained. This was to be used by the classifiers, however not directly. First a list of preprocessing steps are applied on the text. First,  Tokenization is done, which splits a sentence into its words based on punctuations. Stemming, performed next, is the process of getting the root form of a word, generally done based on a set of rules[4]. The stemmer used is the Porter Stemmer[5], developed in 1980 by Martin Porter. Here are some examples of words before and after stemming. Reading becomes read, closer becomes close. After this, all stop words are removed. These are words that are generally used as connectors between the main words in the text. A standard list of English language stopwords that are present in NLTK was used[2]. The next step performed is the replacement of occurrences of “not” + a word with the antonym of the word.
Every classifier requires a feature that is used in the classification. The feature used is the presence of a word(s). Thus, each class is determined by the presence of words that determine it, which is represented as a list of words, each mapped to the boolean True. This is called a bag of words model[6]. For example “Nuclear Reactor” would become {“nuclear”:True, ”reactor”:True}. The classifier used is the Naive Bayes Classifier, and this method of choosing features is best one for the Naive Bayes Classifier[7].
Multiple levels of classifiers were used. To get the hundreds digit, a 10 class classifier was used. For the tens digit, ten 10 class classifiers were created, and depending on the output of the hundreds classifier, one of these 10 were used. Similarly, hundred 10 class classifiers were created for the units digit, and depending on the hundreds and tens digit, one of these hundred were used. These 111 classifiers are already trained and stored in files, and loaded appropriately. 

## 3. Classifying a book - 
The user gives answers to the following questions about the book: the main topic/title of the book, whether the book is about a place, language, religion or ethnic group. The first question is used to determine the first part of the DDC code, while the rest of the questions are used to determine the second part of the code.
The answers to the questions are taken, tokenization, stemming, removal of stop words performed, and are converted to a bag of words form. The 111 classifiers are opened and the user input(in the bag of words form) is classified. Thus, the hundreds, tens and units digits are obtained. Based on this, all the classes that are possible for the fractional part are taken, and a classifier is built on the fly and classification is done. This classifier is on an average 40 classes. This was built on the fly, and not stored, as there will be around 1000 classifiers to store and the space penalty will be high.
For the second part, the answers to the remaining questions are taken. Each question corresponds to one of the auxiliary tables that are part of the DDC schedule[1]. The user's input is directly compared to the values in the tables, and the class which matches most the user's input is chosen. This may not be an elegant approach, however using classifiers here is a problem as the number of output classes is very high. Thus, by appending the output of each of the steps, we get the final DDC code for the book.

# Challenges
A large number of challenges were faced along the way. Originally, it was planned to use existing scanned copies of the DDC book. The problem with this was that the lines of text were not straight, but a bit slanting. To a human, it looked like the text was in a straight line, however when a machine tried to process it, it was not proper, with the next line starting half-way through the current line etc.. This is why another copy was scanned manually and the text recognition software used.
Originally, it was intended that the highest resolution PNG file be used(around 2000 pixels per square inch). This resulted in the OCR part taking too long per page. Hence the resolution was toned down a little bit to make the processing work better. There were very few differences between a high resolution and a low resolution image, and hence it was chosen to use the lower resolution image for further processing.
Before the images were fed to the text recognition software, they needed to be split, as each page had 2 columns of text. Odd and even numbered pages needed different limits specified as they were scanned differently. Even after all of this, the text recognition software threw up some junk characters in the final output. A lot of steps were taken to resolve this problem. The algorithm that would take the text and generate the class to DDC mapping faced problems due to these junk characters. Class descriptions of multiple classes got added on to few specific classes. Hence some of the training data was useless, and this affected the overall performance of the classifier. 

# Conclusion
The system works remarkably well for certain test data. However, whenever it is doubtful, it outputs one standard class as the result – 621.411. This is most likely due to the problem with the text recognition part that was mentioned. A large volume of junk training data was added on to this class, hence many test cases give this as their answer.

The aim of this project was to develop a system that will automate the task that a librarian does. Getting the training data was a big hurdle to cross, and the method used here didn't work for all the images properly. Ignoring this fact, the classifier did work very well. If the data extraction step could be improved, the classifier will work much better and could be used in a production environment.
# References
1. Melvil, Dewey. "Dewey Decimal Classification & Relative Index." (1989).
2. Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O’Reilly Media Inc.
3. GitHub – tesseract-ocr/tesseract: Tesseract Open Source OCR Engine https://github.com/tesseract-ocr/tesseract
4. Porter Stemmer - Program, Vol 14 no. 3 pp 130-137, July 1980
5. Manning, Raghavan and Schutze (2008), Introduction to Information Retrieval. Cambridge University Press
6. Perkins (2014), Python 3 Text Processing with NLTK 3 Cookbook. Packt Publishing
7. Christopher Bishop. Pattern Recognition and Machine Learning. 2e.
